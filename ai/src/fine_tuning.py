import os
import torch
from tqdm import tqdm
from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config
from torch.optim import AdamW
from torch.utils.data import Dataset, DataLoader
from transformers import get_linear_schedule_with_warmup
import matplotlib.pyplot as plt
from datetime import datetime
from torch.amp import GradScaler, autocast
import json
import sys

import os
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

sys.path.append(os.path.join(project_root, 'ai', 'src'))  # ai/src ê²½ë¡œ ì¶”ê°€

# ai ë””ë ‰í† ë¦¬ ë‚´ë¶€ì˜ logs ë° models í´ë” ìƒì„±
os.makedirs(os.path.join(project_root, 'logs'), exist_ok=True)
os.makedirs(os.path.join(project_root, 'models'), exist_ok=True)


from config.fine_tuning_config import ModelConfig
from utils.data_loader import load_training_data

class QueryDataset(Dataset):
    def __init__(self, input_texts, output_texts, tokenizer, max_length):
        self.input_texts = input_texts
        self.output_texts = output_texts
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.input_texts)

    def __getitem__(self, idx):
        input_text = self.input_texts[idx]
        output_text = self.output_texts[idx]

        input_encoding = self.tokenizer(
            input_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors="pt"
        )

        target_encoding = self.tokenizer(
            output_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors="pt"
        )

        return {
            'input_ids': input_encoding.input_ids.squeeze(0),
            'attention_mask': input_encoding.attention_mask.squeeze(0),
            'labels': target_encoding.input_ids.squeeze(0)
        }

    @staticmethod
    def remove_special_tokens(text: str) -> str:
        """<pad>ì™€ </s> í† í° ì œê±°"""
        return text.replace('<pad>', '').replace('</s>', '').replace('<unk>', '')
    
    @staticmethod
    def remove_all_spaces(text: str) -> str:
        """ëª¨ë“  ì¢…ë¥˜ì˜ ê³µë°± ë¬¸ì ì œê±°"""
        return ''.join(text.split())
    
    @staticmethod
    def normalize_spaces(text: str) -> str:
        """ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ë°˜í™˜"""
        return ' '.join(text.split())


class TrainingTracker:
    def __init__(self, log_dir):
        self.log_dir = log_dir
        self.status_file = os.path.join(self.log_dir, 'training_status.json')

        if os.path.exists(self.status_file):
            with open(self.status_file, 'r') as f:
                status = json.load(f)
                self.best_loss = status.get('best_loss', float('inf'))
        else:
            self.best_loss = float('inf')
            
        self.training_history = {
            'epochs': [],
            'train_loss': [],
            'improvement': [],
        }
        self.best_loss = float('inf')
    
    def update(self, epoch, train_loss):
        self.training_history['epochs'].append(epoch)
        self.training_history['train_loss'].append(train_loss)
        
        improved = train_loss < self.best_loss
        self.training_history['improvement'].append(improved)
        if improved:
            self.best_loss = train_loss
        
        self.plot_progress()
        self.save_status(epoch)
        
        return improved
    
    def plot_progress(self):
        plt.figure(figsize=(12, 6))
        plt.plot(self.training_history['epochs'], 
                self.training_history['train_loss'], 
                label='Training Loss', 
                marker='o')
        
        improved_epochs = [e for i, e in enumerate(self.training_history['epochs']) 
                         if self.training_history['improvement'][i]]
        improved_losses = [l for i, l in enumerate(self.training_history['train_loss']) 
                         if self.training_history['improvement'][i]]
        
        if improved_epochs:
            plt.scatter(improved_epochs, improved_losses, 
                       color='green', s=100, 
                       label='Improvement', 
                       zorder=5)
        
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training Progress')
        plt.legend()
        plt.grid(True)
        
        # í˜„ì¬ ì‹œê°„ ì¶”ê°€í•˜ì—¬ í›ˆë ¨ ì§„í–‰ ê·¸ë˜í”„ íŒŒì¼ëª… ì„¤ì •
        current_time = datetime.now().strftime('%Y%m%d_%H%M%S')
        epoch_progress_file = f'training_progress_{current_time}.png'        
        plt.savefig(epoch_progress_file)
        plt.close()
    
    def save_status(self, epoch):
        status = {
            'current_epoch': self.training_history['epochs'][-1],
            'best_loss': self.best_loss,
            'last_train_loss': self.training_history['train_loss'][-1],
            'total_improvements': sum(self.training_history['improvement'])
        }
        
        # ì—í¬í¬ ë²ˆí˜¸ë¥¼ íŒŒì¼ëª…ì— ì¶”ê°€í•˜ì—¬ í›ˆë ¨ ìƒíƒœ ì €ì¥
        current_time = datetime.now().strftime('%Y%m%d_%H%M%S')
        epoch_status_file = os.path.join(self.log_dir, f'training_status_epoch_{epoch}_{current_time}.json')
        status_dir = os.path.dirname(epoch_status_file)
        if status_dir:  # Only create directory if path contains a directory
            os.makedirs(status_dir, exist_ok=True)

        with open(epoch_status_file, 'w') as f:
            json.dump(status, f, indent=4)

def fine_tune_model():
    config = ModelConfig()  # ì´ ë¶€ë¶„ì„ ë¨¼ì € ì„ ì–¸
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    model_path = os.path.join(project_root, 'models', 'fine_tuned_model')
    
    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ë° checkpoint ê²½ë¡œ ì„¤ì •
    model_dir = os.path.join(project_root, 'models', 'best_model')
    checkpoint_path = os.path.join(model_dir, "model_checkpoint.pt")
    
    # ê²½ë¡œ ì¡´ì¬ í™•ì¸
    if not os.path.exists(model_path):
        print(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
        print("ë¨¼ì € train.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•´ì£¼ì„¸ìš”.")
        return
        
    if not os.path.exists(checkpoint_path):
        print(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {checkpoint_path}")
        print("ë¨¼ì € train.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•´ì£¼ì„¸ìš”.")
        return
    
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
    tokenizer = T5Tokenizer.from_pretrained(model_dir)
    print(f"Loaded tokenizer from {model_dir}")

    model = T5ForConditionalGeneration.from_pretrained(config.MODEL_NAME)
    model.resize_token_embeddings(len(tokenizer))
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
        
    print(f"í•™ìŠµëœ ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (Device: {device})")

    # ë°ì´í„°ì…‹ ë¡œë“œ
    data_file = os.path.join(project_root, 'data', 'augmented_dataset.json')
    input_texts, output_texts = load_training_data(data_file)
    
    if len(input_texts) == 0 or len(output_texts) == 0:
        return None, None

    # ë°ì´í„°ì…‹ ì„¤ì •
    train_dataset = QueryDataset(input_texts, output_texts, tokenizer, config.MAX_LENGTH)
    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=config.BATCH_SIZE, 
        shuffle=True, 
        num_workers=2, 
        pin_memory=True if torch.cuda.is_available() else False
    )

    # optimizer ì´ˆê¸°í™” ë° ìƒíƒœ ë¡œë“œ
    optimizer = AdamW(
        model.parameters(), 
        lr=config.LEARNING_RATE,
        weight_decay=config.WEIGHT_DECAY, 
        eps=1e-8
    )
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    # scheduler ì´ˆê¸°í™” ë° ìƒíƒœ ë¡œë“œ
    total_steps = len(train_dataloader) * config.NUM_EPOCHS
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(total_steps * 0.1),
        num_training_steps=total_steps
    )
    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

    # scaler ì´ˆê¸°í™” ë° ìƒíƒœ ë¡œë“œ
    scaler = GradScaler()
    scaler.load_state_dict(checkpoint['scaler_state_dict'])

    # í›ˆë ¨ ìƒíƒœ ì¶”ì ê¸° ì´ˆê¸°í™”
    tracker = TrainingTracker(os.path.join(project_root, 'logs'))
    
    # early stopping ê´€ë ¨ ë³€ìˆ˜ ì´ˆê¸°í™”
    patience = config.PATIENCE
    no_improve = 0
    start_epoch = checkpoint['epoch'] + 1
    best_loss = float('inf')

    print("ğŸš€ Resuming fine-tuning from epoch", start_epoch)
    
    # ì—¬ê¸°ì„œë¶€í„° training loop ì‹œì‘ (ê¸°ì¡´ epoch ë£¨í”„ë¥¼ start_epochë¶€í„° ì‹œì‘)
    for epoch in range(start_epoch, start_epoch + config.NUM_EPOCHS):
        model.train()
        total_train_loss = 0
        train_steps = 0
        
        progress_bar = tqdm(train_dataloader, desc=f"Epoch {epoch+1}")
        optimizer.zero_grad()

        for i, batch in enumerate(progress_bar):
            with autocast(device_type='cuda'):
                output = model(
                    input_ids=batch['input_ids'].to(device),
                    attention_mask=batch['attention_mask'].to(device), 
                    labels=batch['labels'].to(device)
                )
            
                loss = output.loss / config.ACCUMULATION_STEPS
            
            scaler.scale(loss).backward()

            if (i + 1) % config.ACCUMULATION_STEPS == 0:
                scaler.unscale_(optimizer)  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì „ì— ìŠ¤ì¼€ì¼ë§ í•´ì œ
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.GRADIENT_CLIP)
                scaler.step(optimizer)  # Optimizer ì—…ë°ì´íŠ¸
                scaler.update()  # ìŠ¤ì¼€ì¼ëŸ¬ ì—…ë°ì´íŠ¸
                scheduler.step()  # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                optimizer.zero_grad()

            # ì‹¤ì œ ì†ì‹¤ê°’ ì €ì¥
            total_train_loss += loss.item() * config.ACCUMULATION_STEPS
            train_steps += 1

            # ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_bar.set_postfix({
                'loss': f'{loss.item() * config.ACCUMULATION_STEPS:.4f}',
                'lr': f'{scheduler.get_last_lr()[0]:.2e}'
            })
        
        avg_train_loss = total_train_loss / train_steps

        print(f"\nEpoch {epoch+1}")
        print(f"Average Training Loss: {avg_train_loss:.4f}")

        print("\n=== Test Generation ===")
        model.eval()
        test_texts = [
            "Load second recent txn",
            "Get second recent transaction",
            "Show second recent result"
        ]

        for test_text in test_texts:
            inputs = tokenizer(test_text, return_tensors="pt", padding=True, truncation=True).to(device)
            
            with torch.no_grad():
                outputs = model.generate(
                    input_ids=inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    max_length=100,
                    num_beams=10,  # ì¦ê°€
                    length_penalty=0.6,  # ì¶”ê°€
                    no_repeat_ngram_size=2,
                    early_stopping=True,
                    bad_words_ids=[[tokenizer.encode(word, add_special_tokens=False)[0]] for word in ['-src', 'pital']]  # ì˜ëª»ëœ í† í° ë°©ì§€
                )
            
            predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"Input: {test_text}")
            print(f"Output: {predicted_text}\n")

        # íŠ¸ë˜ì»¤ì— ì €ì¥
        improved = tracker.update(epoch + 1, avg_train_loss)
        
        if improved:
            print(f"âœ¨ New best loss achieved!")
            # ëª¨ë¸ ì €ì¥
            checkpoint = {
                'model_state_dict': model.state_dict(),
                'tokenizer_vocab': tokenizer.get_vocab(),  # ì–´íœ˜ ì €ì¥
                'tokenizer_special_tokens_map': tokenizer.special_tokens_map,  # íŠ¹ìˆ˜ í† í° ë§µ ì €ì¥
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'scaler_state_dict': scaler.state_dict(),
                'epoch': epoch,
                'loss': avg_train_loss,
            }

            output_dir = os.path.join(project_root, 'models', 'fine_tuned_model')
            os.makedirs(output_dir, exist_ok=True)
            torch.save(checkpoint, os.path.join(output_dir, 'model_checkpoint.pt'))
            
            model.config.save_pretrained(output_dir)
            tokenizer.save_pretrained(output_dir)
            
            print(f"Saved checkpoint to {output_dir}")
            tracker.save_status(epoch+1)
            no_improve = 0
        else:
            no_improve += 1
            print(f"No improvement for {no_improve} epochs")
            if no_improve >= patience:
                print("ğŸ›‘ Early stopping triggered!")
                break


    return model, tokenizer 


if __name__ == "__main__":
    print("ğŸš€ Starting fine-tuning process...")
    model, tokenizer = fine_tune_model()

    if model is not None and tokenizer is not None:
        print("âœ… Fine-tuning completed successfully!")
    else:
        print("âŒ Fine-tuning failed!")
